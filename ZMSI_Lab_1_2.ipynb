{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOksoRsTrCcYkF9b3SVWyjk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AMMXLENOd7V-"},"source":["<center>\n","    <font size=\"5\"> Zaawansowane Metody Sztucznej Inteligencji<br/>\n","        <small><em>Studia stacjonarne II stopnia 2024/2025</em><br/>Kierunek: Informatyka</small>\n","    </font>\n","</center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"4ZZF68t-kiQ8"},"source":["# Laboratorium nr 1.2: Sztuczne Sieci Neuronowe - Lab 2"]},{"cell_type":"markdown","metadata":{"id":"cwgxhLoJ1uGq"},"source":["## Wielowarstwowe sieci jednokierunkowe\n","Cechą charakterystyczną sieci jednokierunkowych wielowarstwowych jest występowanie, co najmniej jednej warstwy ukrytej neuronów, która pośredniczy w przekazywaniu sygnałów między węzłami w warstwie wejściowej, a warstwą wyjściową."]},{"cell_type":"markdown","metadata":{"id":"xTpCaibN1Ccp"},"source":["### Architektura\n","![MLPnet.gif](http://torus.uck.pk.edu.pl/~amarsz/images/MLPnet.gif)\n","- Sygnały wejściowe podawane są na pierwszą warstwę ukrytą neuronów, a te z kolei stanowią sygnały źródłowe dla kolejnej warstwy itd. aż do warstwy wyjściowej.\n","- W sieci tej występują zazwyczaj pełne połączenia między warstwami tzn. każdy neuron następnej warstwy jest połączony z każdym neuronem warstwy poprzedniej.\n","- Neurony warstw ukrytych stanowią bardzo istotny element sieci, umożliwiający uwzględnienie związków między sygnałami, wynikającymi z zależności statystycznych wyższego rzędu."]},{"cell_type":"markdown","metadata":{"id":"rx6b2MiJ2tig"},"source":["### Uczenie\n","Uczenie sieci MLP odbywa się zwykle z nauczycielem, a najpopularniejszą metodą uczenia jest algorytm wstecznej propagacji błędu. Algorytm wstecznej propagacji - BackPropagation (BP) określa strategię doboru wag w sieci wielowarstwowej przy wykorzystaniu gradientowych metod optymalizacji. Podczas procesu uczenia sieci dokonuje się prezentacji pewnej ilości zestawów uczących (tzn. wektorów wejściowych oraz odpowiadających im wektorów sygnałów wzorcowych (wyjściowych)). Uczenie polega na takim doborze wag neuronów by w efekcie końcowy błąd popełniany przez sieć był mniejszy od zadanego."]},{"cell_type":"markdown","metadata":{"id":"9B7YUblnktrz"},"source":["## Ciągłe funkcje aktywacji. Neurony sigmoidalne.\n","Wykorzystując do uczenia algorytm wstecznej propagacji błędu należy obliczyć pochodne cząstkowe względem wag, co zmusza nas do stosowanie ciągłych (różniczkowalnych) funkcji aktywacji. Najczęściej stosowane są wówczas _neurony sigmoidalne_, których budowa jest prawie taka sama jak perceptronu z tą różnicą, że zamiast progowej funkcji aktywacji wykorzystuje sie ciągłe funkcje sigmoidalne (unipolarną lub bipolarną).\n","\n","Funkcje sigmoidalne odpowiednio unipolarna i bipolarna:\n","\n","![sigmoidalne.png](http://torus.uck.pk.edu.pl/~amarsz/images/sigmoidalne.png)\n","\n","Parametr $\\beta$ pozwala na modyfikację nachylenia funkcji sigmoidalnych.\n","\n","Popularność tych funkcji spowodowana jest łatwością obliczania ich pochodnych, co jest konieczne przy użyciu algorytmów uczenia opartych na podejściu gradientowym. Wartości pochodnych dla powyższych funkcji łatwo obliczyć znając jedynie wartości samych funkcji.\n","$$f'_1(x)=\\frac{\\beta e^{-\\beta x}}{\\left(1-e^{-\\beta x}\\right)^2}=\\beta f_1(x)\\left(1-f_1(x)\\right)$$\n","\n","$$f'_2(x)=\\beta-\\beta\\left(\\frac{\\left(e^{\\beta x}-e^{-\\beta x}\\right)^2}{\\left(e^{\\beta x}+e^{-\\beta x}\\right)^2}\\right)=\\beta \\left(1-f_2(x)^2\\right)$$\n"]},{"cell_type":"markdown","metadata":{"id":"7IZKS6bzcVFM"},"source":["## Algorytm wstecznej propagacji błędu\n","Dla uproszczenia rozważymy przypadek sieci z jedną warstwą ukrytą, której schemat wraz z oznaczeniami przedstawia poniższy rysunek.\n","![mlp1h.png](http://torus.uck.pk.edu.pl/~amarsz/images/mlp1h.png)\n","\n","- Wektor sygnałów wejściowych $x=[1, x_1, x_2, \\ldots, x_N]$.\n","- Macierz $W^{(1)}$ o rozmiarach $K\\times(N+1)$ zawiera wartości wag między sygnałami wejściowymi a warstwą ukrytą, gdzie $w^{(1)}_{kn}$ oznacza wagę pomiędzy $k$-tym neuronem warstwy ukrytej a $n$-tym sygnałem wejściowym.\n","- Macierz $W^{(2)}$ o rozmiarach $M\\times(K+1)$ zawiera wartości wag między warstwą ukrytą a warstwą wyjąciową, gdzie $w^{(2)}_{mk}$ oznacza wagę pomiędzy $m$-tym neuronem warstwy wyjściowej a $k$-tym neuronem warstwy ukrytej.\n","- Wektor pożądanych odpowiedzi $x=[d_1, d_2, \\ldots, d_M]$.\n","\n","Wówczas błąd dla jednego przykładu uczącego wyraża się wzorem:\n","\n","$$E(W) = \\frac{1}{2}\\sum\\limits_{m=1}^M\\left[y_m - d_m\\right]^2$$\n","\n","przy czym $y_m$ wyraża się wzorem:\n","\n","$$ y_m = G\\left(\\sum\\limits_{k=0}^K w_{mk}^{(2)}\\cdot v_k\\right) = G\\left(\\sum\\limits_{k=0}^K w_{mk}^{(2)}\\cdot F\\left(\\sum\\limits_{n=0}^N{w^{(1)}_{kn}\\cdot x_n}\\right)\\right)$$\n","\n","gdzie funkcje $F$ i $G$ to funkcje aktywacji odpowiednio w warstwie ukrytej i wyjściowej."]},{"cell_type":"markdown","metadata":{"id":"q2vtgnMxcqEP"},"source":["### Aktualizacja wag warstwy wyjściowej $W^{(2)}$\n","Poprawa wag odbywa się w kierunku przeciwnym do gradienu (wektora pochodnych), $\\eta$ - współczynnik uczenia:\n","\n","$$ w^{(2)}_{mk} = w^{(2)}_{mk} - \\eta\\cdot\\frac{\\partial E(W)}{\\partial w^{(2)}_{mk}} $$\n","\n","gdzie\n","\n","$$ \\frac{\\partial E(W)}{\\partial w^{(2)}_{mk}} = (y_m - d_m)\\cdot G'\\left(\\sum\\limits_{k=0}^K w_{mk}^{(2)}\\cdot v_k\\right)\\cdot v_k $$"]},{"cell_type":"markdown","metadata":{"id":"UfawIIuicule"},"source":["### Aktualizacja wag warstwy ukrytej $W^{(1)}$\n","Poprawa wag odbywa według tej samej zasady co poprzednio, $\\eta$ - współczynnik uczenia:\n","\n","$$ w^{(1)}_{kn} = w^{(1)}_{kn} - \\eta\\cdot\\frac{\\partial E(W)}{\\partial w^{(1)}_{kn}} $$\n","\n","gdzie\n","\n","$$ \\frac{\\partial E(W)}{\\partial w^{(1)}_{kn}} = \\sum\\limits_{m=1}^M\\left[(y_m - d_m)\\cdot G'\\left(\\sum\\limits_{k=0}^K w_{mk}^{(2)}\\cdot v_k\\right)\\cdot w^{(2)}_{mk}\\cdot F'\\left(\\sum\\limits_{n=0}^N{w^{(1)}_{kn}\\cdot x_n}\\right)\\cdot x_n\\right]$$"]},{"cell_type":"markdown","metadata":{"id":"u7W2VVjPcydZ"},"source":["## Problem XOR\n","Poniższy rysunek (po lewej) ilustruje spójnik logiczny XOR. Aby zrealizować spójnik XOR za pomocą sieci neuronowej, wewnątrz zakreślonego obszaru sieć powinna odpowiadać sygnałem równym 1, natomiast na zewnątrz sygnałem równym 0.\n","![xor.png](http://torus.uck.pk.edu.pl/~amarsz/images/xor.png)\n","\n","Takiego warunku nie można spełnić stosując podział obszaru przy użyciu jednej prostej (jednego neuronu) niezależnie od wartości współczynników tej prostej. Problem XOR nie należy zatem do klasy problemów liniowo separowalnych i nie da się go rozwiązać za pomocą sieci jednowarstwowej.\n","\n","Problem XOR w łatwy sposób daje się rozwiązać oddzielając klasy od siebie za pomocą dwóch prostych. Każda z tych prostych reprezentuje jeden neuron, którego wagi dobieramy tak, aby realizowały podział przestrzeni tak jak na rysunku powyżej (po prawej). Dokładając trzeci neuron, którego wejściami są wyjścia poprzednich neuronów otrzymujemy odpowiedź czy znajdujemy się wewnątrz czy na zewnątrz zaznaczonego obszaru.\n","\n","Przykładowy przebieg uczenia sieci klasyfikującej dane typu XOR.\n","![xor_mlp.gif](http://torus.uck.pk.edu.pl/~amarsz/images/xor_mlp.gif)"]},{"cell_type":"markdown","metadata":{"id":"5xPZCNLQ2zKO"},"source":["## Zadanie 1 (4 pkt., obowiązkowe)\n","- Zaimplementuj (w postaci funkcji) sieć neuronową o architekturze przedstawionej na poniższym rysunku.\n","![zad1.png](http://torus.uck.pk.edu.pl/~amarsz/images/zad1.png)\n","- Dla neuronów warstwy ukrytej jako funkcję aktywacji przyjmij bipolarną funkcję sigmoidalną.\n","- Dla neuronu warstwy wyjściowej jako funkcję aktywacji przyjmij unipolarną funkcję sigmoidalną.\n","- Implementację wykonaj na reprezentacji wektorowo-macierzowej.\n","- Funkcja powinna zwracać odpowiedź sieci oraz wszystkie składniki potrzebne do policzenia pochodnych (patrz zadanie 2)."]},{"cell_type":"code","metadata":{"id":"roJLXMvG4Dgx"},"source":["import numpy as np\n","\n","def sigmoid(x, beta):\n","    return 1.0/(1.0+np.exp(-beta*x))\n","\n","def tanh(x, beta):\n","    return np.tanh(beta*x)\n","\n","# x - sygnał wejściowy [1,x1,x2,...,xN]\n","# w1 - wagi warstwy ukrytej, macierz\n","# w2 - wagi warstwy wyjściowej, wektor\n","# beta - parametr funkcji aktywacji, mogą być dwa różne\n","def mlp(x, w1, w2, beta):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFD4AIrBffbv"},"source":["## Zadanie 2 (6 pkt., obowiązkowe)\n","- Zaimplementuj algorytm wstecznej propagacji błędu dla sieci z zadania 1 w dwóch wariantach:\n","  - aktualizacja wag następuje po każdej próbce uczącej,\n","  - aktualizacja wag następuje po każdej epoce (po wszystkich próbkach uczących).\n","- Stwórz wykres przedstawiające zmiany wartości błędu sieci podczas uczenia (epoka, błąd) rozwiązywania problemu XOR (dane w komórce poniżej).\n","- Oba modele ucz aż błąd klasyfikacji będzie równy 0 (ale nie dłużej niż 100000 epok) przy następującym założeniu\n","  - jeśli odpowiedź sieci jest większa od 0.9 to uznajemy że próbka jest z klasy 1,\n","  - jeśli odpowiedź sieci jest mniejsza od 0.1 to uznajemy że próbka jest z klasy 0,\n","  - w pozostałych przypadkach uznajemy że sieć nie daje jednoznacznej odpowiedzi.\n","- Wskazówka a zarazem pytanie: Aby zbyt często nie utykać w minimach lokalnych, warto zamienić zerowe sygnały wejściowe na wartości niezerowe np. podane niżej dane uczące `xx` warto zamienić na `xx = np.array([[1,-1,-1],[1,-1,1],[1,1,-1],[1,1,1]])`. Dlaczego?"]},{"cell_type":"code","metadata":{"id":"wiUvCib9A6cc"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","xx = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n","#xx = np.array([[1,-1,-1],[1,-1,1],[1,1,-1],[1,1,1]])\n","d = np.array([0, 1, 1, 0])\n","\n","def sigmoid_diff(y, beta):\n","    return beta*y*(1-y)\n","\n","def tanh_diff(y, beta):\n","    return beta*(1-y*y)\n","\n","def train_sample(xx, d, eta, beta):\n","    pass\n","\n","def train_epoach(xx, d, eta, beta):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f2Cuvuo3b9z5"},"execution_count":null,"outputs":[]}]}